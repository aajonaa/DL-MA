{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# DirPLO on CEC2017 Benchmark Functions\n",
    "\n",
    "This notebook demonstrates how to run the DirPLO (Direction-guided Polar Lights Optimizer) algorithm on CEC2017 benchmark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Clear import cache and import PLO\n",
    "import sys\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# Remove cached modules if they exist\n",
    "modules_to_remove = [name for name in sys.modules.keys() if name.startswith('utils') or name == 'PLO' or name == 'optimizer']\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Import PLO and related modules\n",
    "from PLO import DirPLO, OriginalPLO\n",
    "from utils import Problem, FloatVar\n",
    "import cec2017.functions as cec2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "problem-wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class for CEC2017 functions\n",
    "class CEC2017Problem(Problem):\n",
    "    def __init__(self, func_num: int, n_dims: int = 30, **kwargs):\n",
    "        \"\"\"\n",
    "        CEC2017 Problem wrapper\n",
    "        \n",
    "        Args:\n",
    "            func_num: Function number (1-30)\n",
    "            n_dims: Problem dimension (default: 30)\n",
    "        \"\"\"\n",
    "        self.func_num = func_num\n",
    "        self.cec_func = cec2017.all_functions[func_num - 1]  # CEC functions are 0-indexed\n",
    "        \n",
    "        # CEC2017 bounds are typically [-100, 100] for most functions\n",
    "        bounds = FloatVar(lb=[-100.0] * n_dims, ub=[100.0] * n_dims, name=f\"CEC2017_F{func_num}\")\n",
    "        \n",
    "        # Set problem name and other parameters\n",
    "        kwargs.setdefault('name', f'CEC2017_F{func_num}')\n",
    "        kwargs.setdefault('minmax', 'min')\n",
    "        kwargs.setdefault('log_to', 'console')\n",
    "        kwargs.setdefault('save_population', False)\n",
    "        \n",
    "        super().__init__(bounds, **kwargs)\n",
    "    \n",
    "    def obj_func(self, x: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Objective function that calls the CEC2017 function\n",
    "        \n",
    "        Args:\n",
    "            x: Solution vector\n",
    "            \n",
    "        Returns:\n",
    "            Function value\n",
    "        \"\"\"\n",
    "        # CEC2017 functions expect 2D array (1, n_dims)\n",
    "        x_2d = x.reshape(1, -1)\n",
    "        result = self.cec_func(x_2d)\n",
    "        return float(result[0]) if hasattr(result, '__len__') else float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "experiment-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Configuration:\n",
      "  dimensions: [10]\n",
      "  functions: [1, 3, 4, 5, 6]\n",
      "  n_runs: 1\n",
      "  max_epochs: 1000\n",
      "  pop_size: 30\n",
      "  seed_base: 42\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'dimensions': [10],  # Problem dimensions to test\n",
    "    'functions': [1, 3, 4, 5, 6],  # CEC2017 functions to test (subset for demo)\n",
    "    'n_runs': 1,  # Number of independent runs\n",
    "    'max_epochs': 1000,  # Maximum number of iterations\n",
    "    'pop_size': 30,  # Population size\n",
    "    'seed_base': 42  # Base seed for reproducibility\n",
    "}\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(algorithm_class, problem, max_epochs, pop_size, seed=None):\n",
    "    \"\"\"\n",
    "    Run a single optimization experiment\n",
    "    \n",
    "    Args:\n",
    "        algorithm_class: The optimizer class (DirPLO or OriginalPLO)\n",
    "        problem: Problem instance\n",
    "        max_epochs: Maximum number of iterations\n",
    "        pop_size: Population size\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    # Create optimizer instance\n",
    "    optimizer = algorithm_class(epoch=max_epochs, pop_size=pop_size)\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Solve the problem\n",
    "    best_agent = optimizer.solve(problem, seed=seed)\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Extract results\n",
    "    results = {\n",
    "        'best_fitness': best_agent.target.fitness,\n",
    "        'best_solution': best_agent.solution.copy(),\n",
    "        'runtime': end_time - start_time,\n",
    "        'convergence_curve': optimizer.history.list_global_best_fit.copy(),\n",
    "        'algorithm': algorithm_class.__name__,\n",
    "        'function': problem.get_name(),\n",
    "        'dimensions': problem.n_dims,\n",
    "        'seed': seed\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3ac028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "logdir = './logs/'\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "run-experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments...\n",
      "\n",
      "Testing dimension: 10\n",
      "  Function F1:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 1.31e+04\n",
      "      Running OriginalPLO... Best: 4.31e+05\n",
      "  Function F2:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 2.05e+02\n",
      "      Running OriginalPLO... Best: 3.75e+02\n",
      "  Function F3:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 3.00e+02\n",
      "      Running OriginalPLO... Best: 1.90e+03\n",
      "  Function F4:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 4.07e+02\n",
      "      Running OriginalPLO... Best: 4.02e+02\n",
      "  Function F5:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 5.04e+02\n",
      "      Running OriginalPLO... Best: 5.06e+02\n",
      "  Function F6:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 6.01e+02\n",
      "      Running OriginalPLO... Best: 6.04e+02\n",
      "  Function F7:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 7.16e+02\n",
      "      Running OriginalPLO... Best: 7.27e+02\n",
      "  Function F8:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 8.02e+02\n",
      "      Running OriginalPLO... Best: 8.09e+02\n",
      "  Function F9:\n",
      "    Run 1/1 (seed=42)\n",
      "      Running DirPLO... Best: 9.00e+02\n",
      "      Running OriginalPLO... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m      Running OriginalPLO...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     plo_result = \u001b[43mrun_single_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mOriginalPLO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEXPERIMENT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEXPERIMENT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpop_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     all_results.append(plo_result)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplo_result[\u001b[33m'\u001b[39m\u001b[33mbest_fitness\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(algorithm_class, problem, max_epochs, pop_size, seed)\u001b[39m\n\u001b[32m     19\u001b[39m start_time = time.time()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Solve the problem\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m best_agent = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[32m     25\u001b[39m end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL-MA/optimizer.py:227\u001b[39m, in \u001b[36mOptimizer.solve\u001b[39m\u001b[34m(self, problem, mode, n_workers, termination, starting_solutions, seed)\u001b[39m\n\u001b[32m    224\u001b[39m time_epoch = time.perf_counter()\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m## Evolve method will be called in child class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Update global best solution, the population is sorted or not depended on algorithm's strategy\u001b[39;00m\n\u001b[32m    230\u001b[39m pop_temp, \u001b[38;5;28mself\u001b[39m.g_best = \u001b[38;5;28mself\u001b[39m.update_global_best_agent(\u001b[38;5;28mself\u001b[39m.pop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL-MA/PLO.py:102\u001b[39m, in \u001b[36mOriginalPLO.evolve\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m     99\u001b[39m     pop_new.append(agent)\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.AVAILABLE_MODES:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         agent.target = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mself\u001b[39m.pop[idx] = \u001b[38;5;28mself\u001b[39m.get_better_agent(agent, \u001b[38;5;28mself\u001b[39m.pop[idx], \u001b[38;5;28mself\u001b[39m.problem.minmax)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.AVAILABLE_MODES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL-MA/optimizer.py:351\u001b[39m, in \u001b[36mOptimizer.get_target\u001b[39m\u001b[34m(self, solution, counted)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m counted:\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfe_counter += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DL-MA/utils/problem.py:200\u001b[39m, in \u001b[36mProblem.get_target\u001b[39m\u001b[34m(self, solution)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_target\u001b[39m(\u001b[38;5;28mself\u001b[39m, solution: np.ndarray) -> Target:\n\u001b[32m    193\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m        solution: The real-value solution\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m \u001b[33;03m        The target object\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     objs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Target(objectives=objs, weights=\u001b[38;5;28mself\u001b[39m.obj_weights)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mCEC2017Problem.obj_func\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m x_2d = x.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     37\u001b[39m result = \u001b[38;5;28mself\u001b[39m.cec_func(x_2d)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(result[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__len__\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(result)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PLO import *\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "logdir = './logs/'\n",
    "\n",
    "print(\"Starting experiments...\\n\")\n",
    "\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)\n",
    "\n",
    "for dim in EXPERIMENT_CONFIG['dimensions']:\n",
    "    print(f\"Testing dimension: {dim}\")\n",
    "    \n",
    "    for func_num in range(1, 31):\n",
    "        print(f\"  Function F{func_num}:\")\n",
    "        \n",
    "        # Create problem\n",
    "        problem = CEC2017Problem(func_num=func_num, n_dims=dim)\n",
    "        \n",
    "        for run in range(EXPERIMENT_CONFIG['n_runs']):\n",
    "            seed = EXPERIMENT_CONFIG['seed_base'] + run\n",
    "            print(f\"    Run {run + 1}/{EXPERIMENT_CONFIG['n_runs']} (seed={seed})\")\n",
    "            \n",
    "            # Test DirPLO\n",
    "            print(\"      Running DirPLO...\", end=\" \")\n",
    "            try:\n",
    "                dirplo_result = run_single_experiment(\n",
    "                    DirPLO, problem, \n",
    "                    EXPERIMENT_CONFIG['max_epochs'], \n",
    "                    EXPERIMENT_CONFIG['pop_size'], \n",
    "                    seed\n",
    "                )\n",
    "                all_results.append(dirplo_result)\n",
    "                print(f\"Best: {dirplo_result['best_fitness']:.2e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            \n",
    "            # Test Original PLO for comparison\n",
    "            print(\"      Running OriginalPLO...\", end=\" \")\n",
    "            try:\n",
    "                plo_result = run_single_experiment(\n",
    "                    OriginalPLO, problem, \n",
    "                    EXPERIMENT_CONFIG['max_epochs'], \n",
    "                    EXPERIMENT_CONFIG['pop_size'], \n",
    "                    seed\n",
    "                )\n",
    "                all_results.append(plo_result)\n",
    "                print(f\"Best: {plo_result['best_fitness']:.2e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nAll experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if all_results:\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Algorithm': r['algorithm'],\n",
    "            'Function': r['function'],\n",
    "            'Dimensions': r['dimensions'],\n",
    "            'Run': r['seed'] - EXPERIMENT_CONFIG['seed_base'] + 1,\n",
    "            'Best_Fitness': r['best_fitness'],\n",
    "            'Runtime': r['runtime']\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "    \n",
    "    print(\"Results Summary:\")\n",
    "    print(results_df.groupby(['Algorithm', 'Function', 'Dimensions']).agg({\n",
    "        'Best_Fitness': ['mean', 'std', 'min'],\n",
    "        'Runtime': ['mean', 'std']\n",
    "    }).round(4))\n",
    "else:\n",
    "    print(\"No results to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves\n",
    "if all_results:\n",
    "    # Group results by function and dimension\n",
    "    unique_problems = set((r['function'], r['dimensions']) for r in all_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(unique_problems), 1, figsize=(12, 4 * len(unique_problems)))\n",
    "    if len(unique_problems) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (func_name, dim) in enumerate(unique_problems):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get results for this problem\n",
    "        problem_results = [r for r in all_results if r['function'] == func_name and r['dimensions'] == dim]\n",
    "        \n",
    "        # Group by algorithm\n",
    "        for algorithm in ['DirPLO', 'OriginalPLO']:\n",
    "            alg_results = [r for r in problem_results if r['algorithm'] == algorithm]\n",
    "            \n",
    "            if alg_results:\n",
    "                # Plot convergence curves for each run\n",
    "                for i, result in enumerate(alg_results):\n",
    "                    curve = result['convergence_curve']\n",
    "                    alpha = 0.7 if len(alg_results) > 1 else 1.0\n",
    "                    label = f\"{algorithm}\" if i == 0 else None\n",
    "                    ax.semilogy(curve, alpha=alpha, label=label)\n",
    "        \n",
    "        ax.set_title(f\"{func_name} (D={dim})\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Best Fitness (log scale)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis and comparison\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Compare algorithms\n",
    "    for func_name in set(r['function'] for r in all_results):\n",
    "        for dim in set(r['dimensions'] for r in all_results if r['function'] == func_name):\n",
    "            print(f\"\\n{func_name} (D={dim}):\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Get results for this problem\n",
    "            problem_results = [r for r in all_results if r['function'] == func_name and r['dimensions'] == dim]\n",
    "            \n",
    "            # Group by algorithm\n",
    "            for algorithm in ['DirPLO', 'OriginalPLO']:\n",
    "                alg_results = [r for r in problem_results if r['algorithm'] == algorithm]\n",
    "                \n",
    "                if alg_results:\n",
    "                    fitness_values = [r['best_fitness'] for r in alg_results]\n",
    "                    runtimes = [r['runtime'] for r in alg_results]\n",
    "                    \n",
    "                    print(f\"  {algorithm}:\")\n",
    "                    print(f\"    Best Fitness: {np.min(fitness_values):.2e}\")\n",
    "                    print(f\"    Mean Fitness: {np.mean(fitness_values):.2e} ± {np.std(fitness_values):.2e}\")\n",
    "                    print(f\"    Mean Runtime: {np.mean(runtimes):.2f}s ± {np.std(runtimes):.2f}s\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL COMPARISON (DirPLO vs OriginalPLO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    for func_name in set(r['function'] for r in all_results):\n",
    "        for dim in set(r['dimensions'] for r in all_results if r['function'] == func_name):\n",
    "            dirplo_results = [r['best_fitness'] for r in all_results \n",
    "                             if r['function'] == func_name and r['dimensions'] == dim and r['algorithm'] == 'DirPLO']\n",
    "            plo_results = [r['best_fitness'] for r in all_results \n",
    "                          if r['function'] == func_name and r['dimensions'] == dim and r['algorithm'] == 'OriginalPLO']\n",
    "            \n",
    "            if len(dirplo_results) > 1 and len(plo_results) > 1:\n",
    "                # Wilcoxon signed-rank test\n",
    "                try:\n",
    "                    statistic, p_value = stats.wilcoxon(dirplo_results, plo_results)\n",
    "                    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                    \n",
    "                    improvement = (np.mean(plo_results) - np.mean(dirplo_results)) / np.mean(plo_results) * 100\n",
    "                    \n",
    "                    print(f\"\\n{func_name} (D={dim}):\")\n",
    "                    print(f\"  p-value: {p_value:.4f} {significance}\")\n",
    "                    print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "                except:\n",
    "                    print(f\"\\n{func_name} (D={dim}): Statistical test failed\")\n",
    "else:\n",
    "    print(\"No results available for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
