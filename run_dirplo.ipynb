{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# DirPLO on CEC2017 Benchmark Functions\n",
    "\n",
    "This notebook demonstrates how to run the DirPLO (Direction-guided Polar Lights Optimizer) algorithm on CEC2017 benchmark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Clear import cache and import PLO\n",
    "import sys\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# Remove cached modules if they exist\n",
    "modules_to_remove = [name for name in sys.modules.keys() if name.startswith('utils') or name == 'PLO' or name == 'optimizer']\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Import PLO and related modules\n",
    "from PLO import DirPLO, OriginalPLO\n",
    "from utils import Problem, FloatVar\n",
    "import cec2017.functions as cec2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "problem-wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class for CEC2017 functions\n",
    "class CEC2017Problem(Problem):\n",
    "    def __init__(self, func_num: int, n_dims: int = 30, **kwargs):\n",
    "        \"\"\"\n",
    "        CEC2017 Problem wrapper\n",
    "        \n",
    "        Args:\n",
    "            func_num: Function number (1-30)\n",
    "            n_dims: Problem dimension (default: 30)\n",
    "        \"\"\"\n",
    "        self.func_num = func_num\n",
    "        self.cec_func = cec2017.all_functions[func_num - 1]  # CEC functions are 0-indexed\n",
    "        \n",
    "        # CEC2017 bounds are typically [-100, 100] for most functions\n",
    "        bounds = FloatVar(lb=[-100.0] * n_dims, ub=[100.0] * n_dims, name=f\"CEC2017_F{func_num}\")\n",
    "        \n",
    "        # Set problem name and other parameters\n",
    "        kwargs.setdefault('name', f'CEC2017_F{func_num}')\n",
    "        kwargs.setdefault('minmax', 'min')\n",
    "        kwargs.setdefault('log_to', 'console')\n",
    "        kwargs.setdefault('save_population', False)\n",
    "        \n",
    "        super().__init__(bounds, **kwargs)\n",
    "    \n",
    "    def obj_func(self, x: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Objective function that calls the CEC2017 function\n",
    "        \n",
    "        Args:\n",
    "            x: Solution vector\n",
    "            \n",
    "        Returns:\n",
    "            Function value\n",
    "        \"\"\"\n",
    "        # CEC2017 functions expect 2D array (1, n_dims)\n",
    "        x_2d = x.reshape(1, -1)\n",
    "        result = self.cec_func(x_2d)\n",
    "        return float(result[0]) if hasattr(result, '__len__') else float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "experiment-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Configuration:\n",
      "  dimensions: [10]\n",
      "  functions: [1, 3, 4, 5, 6]\n",
      "  n_runs: 1\n",
      "  max_epochs: 1000\n",
      "  pop_size: 30\n",
      "  seed_base: 42\n"
     ]
    }
   ],
   "source": [
    "# Experiment configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'dimensions': [10],  # Problem dimensions to test\n",
    "    'functions': [1, 3, 4, 5, 6],  # CEC2017 functions to test (subset for demo)\n",
    "    'n_runs': 1,  # Number of independent runs\n",
    "    'max_epochs': 1000,  # Maximum number of iterations\n",
    "    'pop_size': 30,  # Population size\n",
    "    'seed_base': 42  # Base seed for reproducibility\n",
    "}\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "run-single-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(algorithm_class, problem, max_epochs, pop_size, seed=None):\n",
    "    \"\"\"\n",
    "    Run a single optimization experiment\n",
    "    \n",
    "    Args:\n",
    "        algorithm_class: The optimizer class (DirPLO or OriginalPLO)\n",
    "        problem: Problem instance\n",
    "        max_epochs: Maximum number of iterations\n",
    "        pop_size: Population size\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    # Create optimizer instance\n",
    "    optimizer = algorithm_class(epoch=max_epochs, pop_size=pop_size)\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Solve the problem\n",
    "    best_agent = optimizer.solve(problem, seed=seed)\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Extract results\n",
    "    results = {\n",
    "        'best_fitness': best_agent.target.fitness,\n",
    "        'best_solution': best_agent.solution.copy(),\n",
    "        'runtime': end_time - start_time,\n",
    "        'convergence_curve': optimizer.history.list_global_best_fit.copy(),\n",
    "        'algorithm': algorithm_class.__name__,\n",
    "        'function': problem.get_name(),\n",
    "        'dimensions': problem.n_dims,\n",
    "        'seed': seed\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ac028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "logdir = './log/'\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PLO import *\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "logdir = './log/'\n",
    "\n",
    "print(\"Starting experiments...\\n\")\n",
    "\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)\n",
    "\n",
    "for dim in EXPERIMENT_CONFIG['dimensions']:\n",
    "    print(f\"Testing dimension: {dim}\")\n",
    "    \n",
    "    for func_num in range(1, 31):\n",
    "        print(f\"  Function F{func_num}:\")\n",
    "        \n",
    "        # Create problem\n",
    "        problem = CEC2017Problem(func_num=func_num, n_dims=dim)\n",
    "        \n",
    "        for run in range(EXPERIMENT_CONFIG['n_runs']):\n",
    "            seed = EXPERIMENT_CONFIG['seed_base'] + run\n",
    "            print(f\"    Run {run + 1}/{EXPERIMENT_CONFIG['n_runs']} (seed={seed})\")\n",
    "            \n",
    "            # Test DirPLO\n",
    "            print(\"      Running DirPLO...\", end=\" \")\n",
    "            try:\n",
    "                dirplo_result = run_single_experiment(\n",
    "                    DirPLO, problem, \n",
    "                    EXPERIMENT_CONFIG['max_epochs'], \n",
    "                    EXPERIMENT_CONFIG['pop_size'], \n",
    "                    seed\n",
    "                )\n",
    "                all_results.append(dirplo_result)\n",
    "                print(f\"Best: {dirplo_result['best_fitness']:.2e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            \n",
    "            # Test Original PLO for comparison\n",
    "            print(\"      Running OriginalPLO...\", end=\" \")\n",
    "            try:\n",
    "                plo_result = run_single_experiment(\n",
    "                    OriginalPLO, problem, \n",
    "                    EXPERIMENT_CONFIG['max_epochs'], \n",
    "                    EXPERIMENT_CONFIG['pop_size'], \n",
    "                    seed\n",
    "                )\n",
    "                all_results.append(plo_result)\n",
    "                print(f\"Best: {plo_result['best_fitness']:.2e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nAll experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if all_results:\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Algorithm': r['algorithm'],\n",
    "            'Function': r['function'],\n",
    "            'Dimensions': r['dimensions'],\n",
    "            'Run': r['seed'] - EXPERIMENT_CONFIG['seed_base'] + 1,\n",
    "            'Best_Fitness': r['best_fitness'],\n",
    "            'Runtime': r['runtime']\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "    \n",
    "    print(\"Results Summary:\")\n",
    "    print(results_df.groupby(['Algorithm', 'Function', 'Dimensions']).agg({\n",
    "        'Best_Fitness': ['mean', 'std', 'min'],\n",
    "        'Runtime': ['mean', 'std']\n",
    "    }).round(4))\n",
    "else:\n",
    "    print(\"No results to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves\n",
    "if all_results:\n",
    "    # Group results by function and dimension\n",
    "    unique_problems = set((r['function'], r['dimensions']) for r in all_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(unique_problems), 1, figsize=(12, 4 * len(unique_problems)))\n",
    "    if len(unique_problems) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (func_name, dim) in enumerate(unique_problems):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get results for this problem\n",
    "        problem_results = [r for r in all_results if r['function'] == func_name and r['dimensions'] == dim]\n",
    "        \n",
    "        # Group by algorithm\n",
    "        for algorithm in ['DirPLO', 'OriginalPLO']:\n",
    "            alg_results = [r for r in problem_results if r['algorithm'] == algorithm]\n",
    "            \n",
    "            if alg_results:\n",
    "                # Plot convergence curves for each run\n",
    "                for i, result in enumerate(alg_results):\n",
    "                    curve = result['convergence_curve']\n",
    "                    alpha = 0.7 if len(alg_results) > 1 else 1.0\n",
    "                    label = f\"{algorithm}\" if i == 0 else None\n",
    "                    ax.semilogy(curve, alpha=alpha, label=label)\n",
    "        \n",
    "        ax.set_title(f\"{func_name} (D={dim})\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Best Fitness (log scale)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis and comparison\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Compare algorithms\n",
    "    for func_name in set(r['function'] for r in all_results):\n",
    "        for dim in set(r['dimensions'] for r in all_results if r['function'] == func_name):\n",
    "            print(f\"\\n{func_name} (D={dim}):\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Get results for this problem\n",
    "            problem_results = [r for r in all_results if r['function'] == func_name and r['dimensions'] == dim]\n",
    "            \n",
    "            # Group by algorithm\n",
    "            for algorithm in ['DirPLO', 'OriginalPLO']:\n",
    "                alg_results = [r for r in problem_results if r['algorithm'] == algorithm]\n",
    "                \n",
    "                if alg_results:\n",
    "                    fitness_values = [r['best_fitness'] for r in alg_results]\n",
    "                    runtimes = [r['runtime'] for r in alg_results]\n",
    "                    \n",
    "                    print(f\"  {algorithm}:\")\n",
    "                    print(f\"    Best Fitness: {np.min(fitness_values):.2e}\")\n",
    "                    print(f\"    Mean Fitness: {np.mean(fitness_values):.2e} ± {np.std(fitness_values):.2e}\")\n",
    "                    print(f\"    Mean Runtime: {np.mean(runtimes):.2f}s ± {np.std(runtimes):.2f}s\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL COMPARISON (DirPLO vs OriginalPLO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    for func_name in set(r['function'] for r in all_results):\n",
    "        for dim in set(r['dimensions'] for r in all_results if r['function'] == func_name):\n",
    "            dirplo_results = [r['best_fitness'] for r in all_results \n",
    "                             if r['function'] == func_name and r['dimensions'] == dim and r['algorithm'] == 'DirPLO']\n",
    "            plo_results = [r['best_fitness'] for r in all_results \n",
    "                          if r['function'] == func_name and r['dimensions'] == dim and r['algorithm'] == 'OriginalPLO']\n",
    "            \n",
    "            if len(dirplo_results) > 1 and len(plo_results) > 1:\n",
    "                # Wilcoxon signed-rank test\n",
    "                try:\n",
    "                    statistic, p_value = stats.wilcoxon(dirplo_results, plo_results)\n",
    "                    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                    \n",
    "                    improvement = (np.mean(plo_results) - np.mean(dirplo_results)) / np.mean(plo_results) * 100\n",
    "                    \n",
    "                    print(f\"\\n{func_name} (D={dim}):\")\n",
    "                    print(f\"  p-value: {p_value:.4f} {significance}\")\n",
    "                    print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "                except:\n",
    "                    print(f\"\\n{func_name} (D={dim}): Statistical test failed\")\n",
    "else:\n",
    "    print(\"No results available for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
